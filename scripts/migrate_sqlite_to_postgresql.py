#!/usr/bin/env python3
"""
SQLite â†’ PostgreSQL ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸
cosmetics.dbì˜ ëª¨ë“  ë°ì´í„°ë¥¼ PostgreSQLë¡œ ì´ì „
"""

import sqlite3
import asyncpg
import asyncio
import json
import os
import sys
from typing import List, Dict, Any, Optional
from datetime import datetime
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class SQLiteToPostgreSQLMigrator:
    """SQLiteì—ì„œ PostgreSQLë¡œ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜"""
    
    def __init__(self, sqlite_path: str, postgres_config: Dict[str, Any]):
        self.sqlite_path = sqlite_path
        self.postgres_config = postgres_config
        self.sqlite_conn = None
        self.postgres_conn = None
        
    async def connect_databases(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
        # SQLite ì—°ê²°
        if not os.path.exists(self.sqlite_path):
            raise FileNotFoundError(f"SQLite íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.sqlite_path}")
        
        self.sqlite_conn = sqlite3.connect(self.sqlite_path)
        self.sqlite_conn.row_factory = sqlite3.Row
        logger.info(f"SQLite ì—°ê²° ì™„ë£Œ: {self.sqlite_path}")
        
        # PostgreSQL ì—°ê²°
        try:
            self.postgres_conn = await asyncpg.connect(**self.postgres_config)
            logger.info("PostgreSQL ì—°ê²° ì™„ë£Œ")
        except Exception as e:
            logger.error(f"PostgreSQL ì—°ê²° ì‹¤íŒ¨: {e}")
            raise
    
    async def close_connections(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ"""
        if self.sqlite_conn:
            self.sqlite_conn.close()
        if self.postgres_conn:
            await self.postgres_conn.close()
    
    def parse_json_field(self, field_value: str) -> List[Any]:
        """JSON ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        if not field_value or field_value == '[]':
            return []
        
        try:
            parsed = json.loads(field_value)
            return parsed if isinstance(parsed, list) else []
        except (json.JSONDecodeError, TypeError):
            return []
    
    def convert_datetime(self, dt_str: str) -> Optional[datetime]:
        """SQLite datetime ë¬¸ìì—´ì„ Python datetimeìœ¼ë¡œ ë³€í™˜"""
        if not dt_str:
            return None
        
        try:
            # SQLiteì˜ CURRENT_TIMESTAMP í˜•ì‹ ì²˜ë¦¬
            if 'T' in dt_str:
                return datetime.fromisoformat(dt_str.replace('Z', '+00:00'))
            else:
                return datetime.strptime(dt_str, '%Y-%m-%d %H:%M:%S')
        except (ValueError, TypeError):
            return None
    
    async def migrate_products(self):
        """ì œí’ˆ í…Œì´ë¸” ë§ˆì´ê·¸ë ˆì´ì…˜"""
        logger.info("ì œí’ˆ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘...")
        
        # SQLiteì—ì„œ ë°ì´í„° ì¡°íšŒ
        cursor = self.sqlite_conn.cursor()
        cursor.execute("""
            SELECT product_id, name, brand_name, category_code, category_name,
                   primary_attr, tags, image_url, sub_product_name,
                   created_at, updated_at
            FROM products
            ORDER BY product_id
        """)
        
        products = cursor.fetchall()
        logger.info(f"SQLiteì—ì„œ {len(products)}ê°œ ì œí’ˆ ì¡°íšŒ ì™„ë£Œ")
        
        # PostgreSQLì— ë°°ì¹˜ ì‚½ì…
        insert_query = """
            INSERT INTO products (
                product_id, name, brand_name, category_code, category_name,
                primary_attr, tags, image_url, sub_product_name,
                created_at, updated_at
            ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11)
            ON CONFLICT (product_id) DO UPDATE SET
                name = EXCLUDED.name,
                brand_name = EXCLUDED.brand_name,
                category_code = EXCLUDED.category_code,
                category_name = EXCLUDED.category_name,
                primary_attr = EXCLUDED.primary_attr,
                tags = EXCLUDED.tags,
                image_url = EXCLUDED.image_url,
                sub_product_name = EXCLUDED.sub_product_name,
                updated_at = EXCLUDED.updated_at
        """
        
        batch_data = []
        for product in products:
            # JSON í•„ë“œ ë³€í™˜
            tags = self.parse_json_field(product['tags'])
            created_at = self.convert_datetime(product['created_at']) or datetime.now()
            updated_at = self.convert_datetime(product['updated_at']) or datetime.now()
            
            batch_data.append((
                product['product_id'],
                product['name'],
                product['brand_name'],
                product['category_code'],
                product['category_name'],
                product['primary_attr'],
                json.dumps(tags, ensure_ascii=False),  # JSONBë¡œ ì €ì¥
                product['image_url'],
                product['sub_product_name'],
                created_at,
                updated_at
            ))
        
        await self.postgres_conn.executemany(insert_query, batch_data)
        
        # ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸
        max_id = max(product['product_id'] for product in products)
        await self.postgres_conn.execute(
            f"SELECT setval('products_product_id_seq', {max_id})"
        )
        
        logger.info(f"ì œí’ˆ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ: {len(products)}ê°œ")
    
    async def migrate_ingredients(self):
        """ì„±ë¶„ í…Œì´ë¸” ë§ˆì´ê·¸ë ˆì´ì…˜"""
        logger.info("ì„±ë¶„ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘...")
        
        cursor = self.sqlite_conn.cursor()
        cursor.execute("""
            SELECT ingredient_id, korean, english, ewg_grade, is_allergy, is_twenty,
                   skin_type_code, skin_good, skin_bad, limitation, forbidden,
                   purposes, tags, created_at, updated_at
            FROM ingredients
            ORDER BY ingredient_id
        """)
        
        ingredients = cursor.fetchall()
        logger.info(f"SQLiteì—ì„œ {len(ingredients)}ê°œ ì„±ë¶„ ì¡°íšŒ ì™„ë£Œ")
        
        insert_query = """
            INSERT INTO ingredients (
                ingredient_id, korean, english, ewg_grade, is_allergy, is_twenty,
                skin_type_code, skin_good, skin_bad, limitation, forbidden,
                purposes, tags, created_at, updated_at
            ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15)
            ON CONFLICT (ingredient_id) DO UPDATE SET
                korean = EXCLUDED.korean,
                english = EXCLUDED.english,
                ewg_grade = EXCLUDED.ewg_grade,
                is_allergy = EXCLUDED.is_allergy,
                is_twenty = EXCLUDED.is_twenty,
                skin_type_code = EXCLUDED.skin_type_code,
                skin_good = EXCLUDED.skin_good,
                skin_bad = EXCLUDED.skin_bad,
                limitation = EXCLUDED.limitation,
                forbidden = EXCLUDED.forbidden,
                purposes = EXCLUDED.purposes,
                tags = EXCLUDED.tags,
                updated_at = EXCLUDED.updated_at
        """
        
        batch_data = []
        for ingredient in ingredients:
            # JSON í•„ë“œ ë³€í™˜
            purposes = self.parse_json_field(ingredient['purposes'])
            tags = self.parse_json_field(ingredient['tags']) if ingredient['tags'] else []
            created_at = self.convert_datetime(ingredient['created_at']) or datetime.now()
            updated_at = self.convert_datetime(ingredient['updated_at']) or datetime.now()
            
            batch_data.append((
                ingredient['ingredient_id'],
                ingredient['korean'],
                ingredient['english'],
                ingredient['ewg_grade'],
                bool(ingredient['is_allergy']),
                bool(ingredient['is_twenty']),
                ingredient['skin_type_code'],
                ingredient['skin_good'],
                ingredient['skin_bad'],
                ingredient['limitation'],
                ingredient['forbidden'],
                json.dumps(purposes, ensure_ascii=False),  # JSONBë¡œ ì €ì¥
                json.dumps(tags, ensure_ascii=False),      # JSONBë¡œ ì €ì¥
                created_at,
                updated_at
            ))
        
        await self.postgres_conn.executemany(insert_query, batch_data)
        
        # ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸
        max_id = max(ingredient['ingredient_id'] for ingredient in ingredients)
        await self.postgres_conn.execute(
            f"SELECT setval('ingredients_ingredient_id_seq', {max_id})"
        )
        
        logger.info(f"ì„±ë¶„ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ: {len(ingredients)}ê°œ")
    
    async def migrate_product_ingredients(self):
        """ì œí’ˆ-ì„±ë¶„ ê´€ê³„ í…Œì´ë¸” ë§ˆì´ê·¸ë ˆì´ì…˜"""
        logger.info("ì œí’ˆ-ì„±ë¶„ ê´€ê³„ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘...")
        
        cursor = self.sqlite_conn.cursor()
        cursor.execute("""
            SELECT product_id, ingredient_id, ordinal
            FROM product_ingredients
            ORDER BY product_id, ordinal
        """)
        
        relations = cursor.fetchall()
        logger.info(f"SQLiteì—ì„œ {len(relations)}ê°œ ì œí’ˆ-ì„±ë¶„ ê´€ê³„ ì¡°íšŒ ì™„ë£Œ")
        
        insert_query = """
            INSERT INTO product_ingredients (product_id, ingredient_id, ordinal)
            VALUES ($1, $2, $3)
            ON CONFLICT (product_id, ingredient_id) DO UPDATE SET
                ordinal = EXCLUDED.ordinal
        """
        
        batch_data = [
            (relation['product_id'], relation['ingredient_id'], relation['ordinal'])
            for relation in relations
        ]
        
        await self.postgres_conn.executemany(insert_query, batch_data)
        logger.info(f"ì œí’ˆ-ì„±ë¶„ ê´€ê³„ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ: {len(relations)}ê°œ")
    
    async def migrate_goods(self):
        """ìƒí’ˆ í…Œì´ë¸” ë§ˆì´ê·¸ë ˆì´ì…˜"""
        logger.info("ìƒí’ˆ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘...")
        
        cursor = self.sqlite_conn.cursor()
        cursor.execute("""
            SELECT goods_id, product_id, name, price, capacity, sale_status,
                   partner_name, thumbnail_url, created_at, updated_at
            FROM goods
            ORDER BY goods_id
        """)
        
        goods = cursor.fetchall()
        logger.info(f"SQLiteì—ì„œ {len(goods)}ê°œ ìƒí’ˆ ì¡°íšŒ ì™„ë£Œ")
        
        if not goods:
            logger.info("ìƒí’ˆ ë°ì´í„°ê°€ ì—†ì–´ ë§ˆì´ê·¸ë ˆì´ì…˜ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        insert_query = """
            INSERT INTO goods (
                goods_id, product_id, name, price, capacity, sale_status,
                partner_name, thumbnail_url, created_at, updated_at
            ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
            ON CONFLICT (goods_id) DO UPDATE SET
                product_id = EXCLUDED.product_id,
                name = EXCLUDED.name,
                price = EXCLUDED.price,
                capacity = EXCLUDED.capacity,
                sale_status = EXCLUDED.sale_status,
                partner_name = EXCLUDED.partner_name,
                thumbnail_url = EXCLUDED.thumbnail_url,
                updated_at = EXCLUDED.updated_at
        """
        
        batch_data = []
        for good in goods:
            created_at = self.convert_datetime(good['created_at']) or datetime.now()
            updated_at = self.convert_datetime(good['updated_at']) or datetime.now()
            
            batch_data.append((
                good['goods_id'],
                good['product_id'],
                good['name'],
                float(good['price']),
                good['capacity'],
                good['sale_status'],
                good['partner_name'],
                good['thumbnail_url'],
                created_at,
                updated_at
            ))
        
        await self.postgres_conn.executemany(insert_query, batch_data)
        
        # ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸
        max_id = max(good['goods_id'] for good in goods)
        await self.postgres_conn.execute(
            f"SELECT setval('goods_goods_id_seq', {max_id})"
        )
        
        logger.info(f"ìƒí’ˆ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ: {len(goods)}ê°œ")
    
    async def migrate_other_tables(self):
        """ê¸°íƒ€ í…Œì´ë¸”ë“¤ ë§ˆì´ê·¸ë ˆì´ì…˜ (product_metrics, review_topics)"""
        # product_metrics ë§ˆì´ê·¸ë ˆì´ì…˜
        cursor = self.sqlite_conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM product_metrics")
        metrics_count = cursor.fetchone()[0]
        
        if metrics_count > 0:
            logger.info(f"ì œí’ˆ ë©”íŠ¸ë¦­ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘... ({metrics_count}ê°œ)")
            cursor.execute("""
                SELECT product_id, rating_avg, review_count, category_overall_rank,
                       by_attribute_rank, rank_attribute_name, updated_at
                FROM product_metrics
            """)
            
            metrics = cursor.fetchall()
            insert_query = """
                INSERT INTO product_metrics (
                    product_id, rating_avg, review_count, category_overall_rank,
                    by_attribute_rank, rank_attribute_name, updated_at
                ) VALUES ($1, $2, $3, $4, $5, $6, $7)
                ON CONFLICT (product_id) DO UPDATE SET
                    rating_avg = EXCLUDED.rating_avg,
                    review_count = EXCLUDED.review_count,
                    category_overall_rank = EXCLUDED.category_overall_rank,
                    by_attribute_rank = EXCLUDED.by_attribute_rank,
                    rank_attribute_name = EXCLUDED.rank_attribute_name,
                    updated_at = EXCLUDED.updated_at
            """
            
            batch_data = []
            for metric in metrics:
                updated_at = self.convert_datetime(metric['updated_at']) or datetime.now()
                batch_data.append((
                    metric['product_id'],
                    float(metric['rating_avg']) if metric['rating_avg'] else None,
                    metric['review_count'],
                    metric['category_overall_rank'],
                    metric['by_attribute_rank'],
                    metric['rank_attribute_name'],
                    updated_at
                ))
            
            await self.postgres_conn.executemany(insert_query, batch_data)
            logger.info(f"ì œí’ˆ ë©”íŠ¸ë¦­ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ: {len(metrics)}ê°œ")
        
        # review_topics ë§ˆì´ê·¸ë ˆì´ì…˜
        cursor.execute("SELECT COUNT(*) FROM review_topics")
        topics_count = cursor.fetchone()[0]
        
        if topics_count > 0:
            logger.info(f"ë¦¬ë·° í† í”½ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘... ({topics_count}ê°œ)")
            cursor.execute("""
                SELECT id, product_id, sentiment, name, sentence, review_count, score, updated_at
                FROM review_topics
            """)
            
            topics = cursor.fetchall()
            insert_query = """
                INSERT INTO review_topics (
                    id, product_id, sentiment, name, sentence, review_count, score, updated_at
                ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
                ON CONFLICT (id) DO UPDATE SET
                    product_id = EXCLUDED.product_id,
                    sentiment = EXCLUDED.sentiment,
                    name = EXCLUDED.name,
                    sentence = EXCLUDED.sentence,
                    review_count = EXCLUDED.review_count,
                    score = EXCLUDED.score,
                    updated_at = EXCLUDED.updated_at
            """
            
            batch_data = []
            for topic in topics:
                updated_at = self.convert_datetime(topic['updated_at']) or datetime.now()
                batch_data.append((
                    topic['id'],
                    topic['product_id'],
                    topic['sentiment'],
                    topic['name'],
                    topic['sentence'],
                    topic['review_count'],
                    float(topic['score']) if topic['score'] else 0.0,
                    updated_at
                ))
            
            await self.postgres_conn.executemany(insert_query, batch_data)
            
            # ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸
            max_id = max(topic['id'] for topic in topics)
            await self.postgres_conn.execute(
                f"SELECT setval('review_topics_id_seq', {max_id})"
            )
            
            logger.info(f"ë¦¬ë·° í† í”½ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ: {len(topics)}ê°œ")
    
    async def verify_migration(self):
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦"""
        logger.info("ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦ ì‹œì‘...")
        
        # SQLite ë°ì´í„° ê°œìˆ˜ ì¡°íšŒ
        cursor = self.sqlite_conn.cursor()
        
        sqlite_counts = {}
        tables = ['products', 'ingredients', 'product_ingredients', 'goods', 'product_metrics', 'review_topics']
        
        for table in tables:
            cursor.execute(f"SELECT COUNT(*) FROM {table}")
            sqlite_counts[table] = cursor.fetchone()[0]
        
        # PostgreSQL ë°ì´í„° ê°œìˆ˜ ì¡°íšŒ
        postgres_counts = {}
        for table in tables:
            result = await self.postgres_conn.fetchval(f"SELECT COUNT(*) FROM {table}")
            postgres_counts[table] = result
        
        # ê²€ì¦ ê²°ê³¼ ì¶œë ¥
        logger.info("=== ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦ ê²°ê³¼ ===")
        all_match = True
        
        for table in tables:
            sqlite_count = sqlite_counts[table]
            postgres_count = postgres_counts[table]
            match = sqlite_count == postgres_count
            
            if not match:
                all_match = False
            
            status = "âœ“" if match else "âœ—"
            logger.info(f"{status} {table}: SQLite({sqlite_count}) â†’ PostgreSQL({postgres_count})")
        
        if all_match:
            logger.info("âœ“ ëª¨ë“  í…Œì´ë¸” ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜ë˜ì—ˆìŠµë‹ˆë‹¤!")
        else:
            logger.error("âœ— ì¼ë¶€ í…Œì´ë¸”ì—ì„œ ë°ì´í„° ë¶ˆì¼ì¹˜ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return all_match
    
    async def run_migration(self):
        """ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰"""
        try:
            await self.connect_databases()
            
            # ìˆœì„œëŒ€ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
            await self.migrate_products()
            await self.migrate_ingredients()
            await self.migrate_product_ingredients()
            await self.migrate_goods()
            await self.migrate_other_tables()
            
            # ê²€ì¦
            success = await self.verify_migration()
            
            if success:
                logger.info("ğŸ‰ SQLite â†’ PostgreSQL ë§ˆì´ê·¸ë ˆì´ì…˜ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            else:
                logger.error("âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
            return False
        finally:
            await self.close_connections()


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # PostgreSQL ì—°ê²° ì„¤ì • (DATABASE_URL ìš°ì„ , ì—†ìœ¼ë©´ ê°œë³„ í™˜ê²½ë³€ìˆ˜)
    database_url = os.getenv('DATABASE_URL')
    
    if database_url:
        # DATABASE_URL íŒŒì‹±
        from urllib.parse import urlparse
        parsed = urlparse(database_url)
        postgres_config = {
            'host': parsed.hostname,
            'port': parsed.port or 5432,
            'database': parsed.path.lstrip('/'),
            'user': parsed.username,
            'password': parsed.password,
        }
        # SSL ì„¤ì • ì²˜ë¦¬
        if 'sslmode=require' in database_url:
            postgres_config['ssl'] = 'require'
    else:
        # ê°œë³„ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©
        postgres_config = {
            'host': os.getenv('POSTGRES_HOST', 'localhost'),
            'port': int(os.getenv('POSTGRES_PORT', 5432)),
            'database': os.getenv('POSTGRES_DB', 'cosmetics'),
            'user': os.getenv('POSTGRES_USER', 'postgres'),
            'password': os.getenv('POSTGRES_PASSWORD', 'password')
        }
    
    sqlite_path = os.getenv('SQLITE_PATH', 'cosmetics.db')
    
    logger.info("SQLite â†’ PostgreSQL ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
    logger.info(f"SQLite íŒŒì¼: {sqlite_path}")
    logger.info(f"PostgreSQL: {postgres_config['host']}:{postgres_config['port']}/{postgres_config['database']}")
    
    migrator = SQLiteToPostgreSQLMigrator(sqlite_path, postgres_config)
    success = await migrator.run_migration()
    
    if success:
        logger.info("ë§ˆì´ê·¸ë ˆì´ì…˜ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(0)
    else:
        logger.error("ë§ˆì´ê·¸ë ˆì´ì…˜ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())